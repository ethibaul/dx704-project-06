{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Md413FzAvFD8"
      },
      "source": [
        "# DX 704 Week 6 Project\n",
        "\n",
        "This project will develop a treatment plan for a fictious illness \"Twizzleflu\".\n",
        "Twizzleflu is a mild illness caused by a virus.\n",
        "The main symptoms are a mild fever, fidgeting, and kicking the blankets off the bed or couch.\n",
        "Mild dehydration has also been reported in more severe cases.\n",
        "These symptoms typically last 1-2 weeks without treatment.\n",
        "Word on the internet says that Twizzleflu can be cured faster by drinking copious orange juice, but this has not been supported by evidence so far.\n",
        "You will be provided with a theoretical model of Twizzleflu modeled as a Markov decision process.\n",
        "Based on the model, you will compute optimal treatment plans to optimize different criteria, and compare patient discomfort with the different plans."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzyRo9Tw5VcB"
      },
      "source": [
        "The full project description, a template notebook, and raw data are available on GitHub: [Project 6 Materials](https://github.com/bu-cds-dx704/dx704-project-06)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGYOZcnP6Vfu"
      },
      "source": [
        "We will model Twizzleflu as a Markov decision process.\n",
        "The model transition probabilities are provided in the file \"twizzleflu-transitions.tsv\" and the expected rewards are in \"twizzleflu-rewards.tsv\".\n",
        "The goal for Twizzleflu is to minimize the expected discomfort of the patient which is expressed as negative rewards in the file."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dlm2sUsades5"
      },
      "source": [
        "## Example Code\n",
        "\n",
        "You may find it helpful to refer to these GitHub repositories of Jupyter notebooks for example code.\n",
        "\n",
        "* https://github.com/bu-cds-omds/dx601-examples\n",
        "* https://github.com/bu-cds-omds/dx602-examples\n",
        "* https://github.com/bu-cds-omds/dx603-examples\n",
        "* https://github.com/bu-cds-omds/dx704-examples\n",
        "\n",
        "Any calculations demonstrated in code examples or videos may be found in these notebooks, and you are allowed to copy this example code in your homework answers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C8oSLkMqvMFF"
      },
      "source": [
        "## Part 1: Evaluate a Do Nothing Plan\n",
        "\n",
        "One of the treatment actions is to do nothing.\n",
        "Calculate the expected discomfort (not rewards) of a policy that always does nothing."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cvG4mi_sAF9A"
      },
      "source": [
        "Hint: for this value calculation and later ones, use value iteration.\n",
        "The analytical solution has difficulties in practice when there is no discount factor."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "RVfnE8vf8yIl"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def compute_qT_once(R, P, gamma, v):\n",
        "    return R + gamma * P @ v"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "def iterate_values_once(R, P, gamma, v):\n",
        "    return np.max(compute_qT_once(R, P, gamma, v), axis=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "def value_iteration(R, P, gamma, max_iterations=100, tolerance=0.001):\n",
        "    # initial approximation v_0\n",
        "    v_old = np.zeros(R.shape[-1])\n",
        "\n",
        "    for i in range(max_iterations):\n",
        "        # compute v_{i+1}\n",
        "        v_new = iterate_values_once(R, P, gamma, v_old)\n",
        "\n",
        "        # check if values did not change much\n",
        "        if np.max(np.abs(v_new - v_old)) < tolerance:\n",
        "            return v_new\n",
        "\n",
        "        v_old = v_new\n",
        "\n",
        "    # return v_{max_iterations}\n",
        "    return v_old"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "# from example code for reference\n",
        "#gamma=0.9\n",
        "#v_star = value_iteration(R, P, gamma)\n",
        "#v_star"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "States (in order): ['exposed-1', 'exposed-2', 'exposed-3', 'recovered', 'symptoms-1', 'symptoms-2', 'symptoms-3']\n",
            "Expected discomfort per state under 'Do Nothing' (discounted, gamma=1):\n",
            "             exposed-1:  3.410978\n",
            "             exposed-2:  4.264491\n",
            "             exposed-3:  5.331327\n",
            "             recovered: -0.000000\n",
            "            symptoms-1:  6.664819\n",
            "            symptoms-2:  4.999779\n",
            "            symptoms-3:  1.666654\n"
          ]
        }
      ],
      "source": [
        "# set params\n",
        "gamma = 1\n",
        "do_nothing_aliases = {\"do-nothing\"}\n",
        "\n",
        "# load files\n",
        "T = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "Rraw = pd.read_csv(\"twizzleflu-rewards.tsv\",    sep=\"\\t\")\n",
        "T.columns = [c.lower() for c in T.columns]\n",
        "Rraw.columns = [c.lower() for c in Rraw.columns]\n",
        "\n",
        "# Build state/action sets\n",
        "states = sorted(set(T[\"state\"]) | set(T[\"next_state\"]))\n",
        "actions = sorted(set(T[\"action\"]) | set(Rraw[\"action\"]))\n",
        "s2i = {s:i for i,s in enumerate(states)}\n",
        "a2i = {a:i for i,a in enumerate(actions)}\n",
        "S, A = len(states), len(actions)\n",
        "\n",
        "# build transition tensor P: (A, S, S)\n",
        "P = np.zeros((A, S, S), dtype=float)\n",
        "for _, r in T.iterrows():\n",
        "    a = a2i[r[\"action\"]]; s = s2i[r[\"state\"]]; sp = s2i[r[\"next_state\"]]\n",
        "    P[a, s, sp] += float(r[\"probability\"])\n",
        "\n",
        "# build reward matrix R: (A, S)\n",
        "if {\"state\",\"action\",\"reward\"}.issubset(Rraw.columns):\n",
        "    Rsa = Rraw[[\"state\",\"action\",\"reward\"]].copy()\n",
        "else:\n",
        "    raise ValueError(\"Rewards file must have (state, action, reward).\")\n",
        "\n",
        "R = np.zeros((A, S), dtype=float); R[:] = np.nan\n",
        "for _, r in Rsa.iterrows():\n",
        "    a = a2i[r[\"action\"]]; s = s2i[r[\"state\"]]\n",
        "    R[a, s] = float(r[\"reward\"])\n",
        "R = np.nan_to_num(R, nan=0.0)\n",
        "\n",
        "# 'do nothing' action index\n",
        "lower_map = {a.lower(): a for a in actions}\n",
        "do_a_idx = None\n",
        "for alias in do_nothing_aliases:\n",
        "    if alias in lower_map:\n",
        "        do_a_idx = a2i[lower_map[alias]]\n",
        "        break\n",
        "if do_a_idx is None:\n",
        "    raise ValueError(f\"No 'do nothing' action found. Seen actions: {actions}\")\n",
        "\n",
        "# restrict to the do-nothing action\n",
        "R_pi = R[do_a_idx:do_a_idx+1, :]    \n",
        "P_pi = P[do_a_idx:do_a_idx+1, :, :] \n",
        "\n",
        "# run value iteration\n",
        "v_star_rewards = value_iteration(R_pi, P_pi, gamma=gamma)\n",
        "\n",
        "# convert to discomfort (rewards are negative discomfort)\n",
        "discomfort_per_state = -v_star_rewards\n",
        "\n",
        "print(\"States (in order):\", states)\n",
        "print(\"Expected discomfort per state under 'Do Nothing' (discounted, gamma=1):\")\n",
        "for s_name, d in zip(states, discomfort_per_state):\n",
        "    print(f\"  {s_name:>20s}: {float(d): .6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oji9gHEk8ytE"
      },
      "source": [
        "Save the expected discomfort by state to a file \"do-nothing-discomfort.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "ZLDuiAb99ACA"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "do_nothing_discomfort = pd.DataFrame({\n",
        "    \"state\": states,\n",
        "    \"expected_discomfort\": discomfort_per_state\n",
        "})\n",
        "do_nothing_discomfort.to_csv(\"do-nothing-discomfort.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-8sGANC-Dzs"
      },
      "source": [
        "Submit \"do-nothing-discomfort.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RJ1ietVp9BCS"
      },
      "source": [
        "## Part 2: Compute an Optimal Treatment Plan\n",
        "\n",
        "Compute an optimal treatment plan for Twizzleflu.\n",
        "It should minimize the expected discomfort (maximize the rewards)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "6fdjt6qk9mZM"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Optimal treatment plan (policy):\n",
            "  State            exposed-1 -> Action: sleep-8\n",
            "  State            exposed-2 -> Action: sleep-8\n",
            "  State            exposed-3 -> Action: sleep-8\n",
            "  State            recovered -> Action: do-nothing\n",
            "  State           symptoms-1 -> Action: drink-oj\n",
            "  State           symptoms-2 -> Action: drink-oj\n",
            "  State           symptoms-3 -> Action: drink-oj\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# same as above here\n",
        "gamma = 1.0\n",
        "\n",
        "T = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "Rraw = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "T.columns = [c.lower() for c in T.columns]\n",
        "Rraw.columns = [c.lower() for c in Rraw.columns]\n",
        "\n",
        "states = sorted(set(T[\"state\"]) | set(T[\"next_state\"]))\n",
        "actions = sorted(set(T[\"action\"]) | set(Rraw[\"action\"]))\n",
        "s2i = {s:i for i,s in enumerate(states)}\n",
        "a2i = {a:i for i,a in enumerate(actions)}\n",
        "S, A = len(states), len(actions)\n",
        "\n",
        "P = np.zeros((A, S, S), dtype=float)\n",
        "for _, r in T.iterrows():\n",
        "    a = a2i[r[\"action\"]]\n",
        "    s = s2i[r[\"state\"]]\n",
        "    sp = s2i[r[\"next_state\"]]\n",
        "    P[a, s, sp] += float(r[\"probability\"])\n",
        "\n",
        "if {\"state\",\"action\",\"reward\"}.issubset(Rraw.columns):\n",
        "    Rsa = Rraw[[\"state\",\"action\",\"reward\"]].copy()\n",
        "else:\n",
        "    raise ValueError(\"Rewards file must have (state, action, reward).\")\n",
        "\n",
        "R = np.zeros((A, S), dtype=float); R[:] = np.nan\n",
        "for _, r in Rsa.iterrows():\n",
        "    a = a2i[r[\"action\"]]\n",
        "    s = s2i[r[\"state\"]]\n",
        "    R[a, s] = float(r[\"reward\"])\n",
        "R = np.nan_to_num(R, nan=0.0)\n",
        "\n",
        "# value iteration over all actions\n",
        "v_star_rewards = value_iteration(R, P, gamma=gamma)\n",
        "\n",
        "# get optimal greedy policy from Q(a,s)\n",
        "Q_star = compute_qT_once(R, P, gamma, v_star_rewards)  \n",
        "best_action_idx_per_state = np.argmax(Q_star, axis=0)   \n",
        "optimal_actions = [actions[i] for i in best_action_idx_per_state]\n",
        "\n",
        "# convert to expected discomfort (in case we need direct comparison to above at some point)\n",
        "expected_discomfort_opt = -v_star_rewards\n",
        "\n",
        "# print outcome\n",
        "print(\"Optimal treatment plan (policy):\")\n",
        "for s, a in zip(states, optimal_actions):\n",
        "    print(f\"  State {s:>20s} -> Action: {a}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PRcByl1h9nBf"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-discomfort-actions.tsv\" with columns state and action."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "FhAajvpX9wru"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "optimal_policy=pd.DataFrame({\n",
        "    \"state\": states, \n",
        "    \"action\": optimal_actions\n",
        "    })\n",
        "\n",
        "optimal_policy.to_csv(\"minimum-discomfort-actions.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sr00MlhL-Hdv"
      },
      "source": [
        "Submit \"minimum-discomfort-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "65p3NRTy9xjT"
      },
      "source": [
        "## Part 3: Expected Discomfort\n",
        "\n",
        "Using your previous optimal policy, compute the expected discomfort for each state."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "t5bHbK24-AhQ"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Expected discomfort per state:\n",
            "  State            exposed-1 -> Expected discomfort: 0.7491647930150531\n",
            "  State            exposed-2 -> Expected discomfort: 1.4986889192678159\n",
            "  State            exposed-3 -> Expected discomfort: 2.9979439870336204\n",
            "  State            recovered -> Expected discomfort: -0.0\n",
            "  State           symptoms-1 -> Expected discomfort: 5.996778913019338\n",
            "  State           symptoms-2 -> Expected discomfort: 4.499579858219914\n",
            "  State           symptoms-3 -> Expected discomfort: 1.499973182439569\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# Expected discomforts were saved out above in calculating the optimal policy\n",
        "print(\"Expected discomfort per state:\")\n",
        "for s, a in zip(states, expected_discomfort_opt):\n",
        "    print(f\"  State {s:>20s} -> Expected discomfort: {a}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Er6-0c0f-BGw"
      },
      "source": [
        "Save your results in a file \"minimum-discomfort-values.tsv\" with columns state and expected_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "NAQFQnp_-TZ1"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "expected_discomfort=pd.DataFrame({\n",
        "    \"state\": states, \n",
        "    \"expected_discomfort\": expected_discomfort_opt\n",
        "    })\n",
        "\n",
        "expected_discomfort.to_csv(\"minimum-discomfort-values.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83wnFZfk-UDd"
      },
      "source": [
        "Submit \"minimum-discomfort-values.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKUTt9gx-XBF"
      },
      "source": [
        "## Part 4: Minimizing Twizzleflu Duration\n",
        "\n",
        "Modifiy the Markov decision process to minimize the days until the Twizzle flu is over.\n",
        "To do so, change the reward function to always be -1 if the current state corresponds to being sick and 0 if the current state corresponds to being better.\n",
        "To be clear, the action does not matter for this reward function.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "HXrnkCh5-trk"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Expected sick days per state:\n",
            "             exposed-1 -> 1.250000\n",
            "             exposed-2 -> 2.500000\n",
            "             exposed-3 -> 5.000000\n",
            "             recovered -> -0.000000\n",
            "            symptoms-1 -> 10.000000\n",
            "            symptoms-2 -> 6.666667\n",
            "            symptoms-3 -> 3.333333\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# same as above here\n",
        "gamma = 1.0\n",
        "\n",
        "T = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "Rraw = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")  # not used for Part 4 rewards; just to gather actions\n",
        "T.columns = [c.lower() for c in T.columns]\n",
        "Rraw.columns = [c.lower() for c in Rraw.columns]\n",
        "\n",
        "states  = sorted(set(T[\"state\"]) | set(T[\"next_state\"]))\n",
        "actions = sorted(set(T[\"action\"]) | set(Rraw.get(\"action\", pd.Series([], dtype=str))))\n",
        "s2i = {s:i for i,s in enumerate(states)}\n",
        "a2i = {a:i for i,a in enumerate(actions)}\n",
        "\n",
        "S, A = len(states), len(actions)\n",
        "\n",
        "P = np.zeros((A, S, S), dtype=float)\n",
        "for _, r in T.iterrows():\n",
        "    a = a2i[r[\"action\"]]\n",
        "    s = s2i[r[\"state\"]]\n",
        "    sp = s2i[r[\"next_state\"]]\n",
        "    P[a, s, sp] += float(r['probability'])\n",
        "\n",
        "# assign sick to -1\n",
        "sick_states = {\"symptoms-1\", \"symptoms-2\", \"symptoms-3\"}  \n",
        "sick_mask   = np.array([s in sick_states for s in states], dtype=bool)\n",
        "better_mask = ~sick_mask\n",
        "\n",
        "R_duration = np.zeros((A, S), dtype=float)\n",
        "R_duration[:, sick_mask]   = -1.0\n",
        "R_duration[:, better_mask] =  0.0\n",
        "\n",
        "# value_iteration minimizing sick time\n",
        "v_star_rewards = value_iteration(R_duration, P, gamma=gamma, max_iterations=10000, tolerance=1e-8)\n",
        "\n",
        "Q_star = compute_qT_once(R_duration, P, gamma, v_star_rewards)   # shape (A, S)\n",
        "best_action_idx_per_state = np.argmax(Q_star, axis=0)\n",
        "optimal_actions = [actions[i] for i in best_action_idx_per_state]\n",
        "\n",
        "expected_sick_days = -v_star_rewards\n",
        "\n",
        "print(\"\\nExpected sick days per state:\")\n",
        "for s, d in zip(states, expected_sick_days):\n",
        "    print(f\"  {s:>20s} -> {float(d):.6f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "je9Rt239-uRl"
      },
      "source": [
        "Save your new reward function in a file \"duration-rewards.tsv\" in the same format as \"twizzleflu-rewards.tsv\"."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "_cmV1ewj-4-Q"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "duration_rewards_df = []\n",
        "\n",
        "for a in actions:\n",
        "    for s in states:\n",
        "        reward = -1.0 if s in sick_states else 0.0\n",
        "        duration_rewards_df.append({\n",
        "            \"state\": s,\n",
        "            \"action\": a,\n",
        "            \"reward\": reward\n",
        "        })\n",
        "\n",
        "duration_rewards_df = pd.DataFrame(duration_rewards_df)\n",
        "duration_rewards_df.to_csv(\"duration-rewards.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "r0lubs9v-5XQ"
      },
      "source": [
        "Submit \"duration-rewards.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jf73YFzB-802"
      },
      "source": [
        "## Part 5: Optimize for Shorter Twizzleflu\n",
        "\n",
        "Compute an optimal policy to minimize the duration of Twizzleflu."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Sa_HI0f0_FHA"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "# same again\n",
        "gamma = 0.999\n",
        "\n",
        "T = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\")\n",
        "Rraw = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\")\n",
        "T.columns = [c.lower() for c in T.columns]\n",
        "Rraw.columns = [c.lower() for c in Rraw.columns]\n",
        "\n",
        "state_col  = \"state\"         # column giving the current state\n",
        "action_col = \"action\"        # column giving the action\n",
        "prob_col   = \"probability\"   # column giving transition probability\n",
        "\n",
        "# --- Build list of states and actions ---\n",
        "states_from_T = set(T[state_col].unique())\n",
        "states_from_R = set(Rraw[\"state\"].unique()) if \"state\" in Rraw.columns else set()\n",
        "states = sorted(states_from_T | states_from_R)\n",
        "actions = sorted(T[action_col].unique())\n",
        "s2i = {s:i for i,s in enumerate(states)}\n",
        "a2i = {a:i for i,a in enumerate(actions)}\n",
        "S, A = len(states), len(actions)\n",
        "\n",
        "# --- Build transition tensor P: (A,S,S) ---\n",
        "P = np.zeros((A, S, S), dtype=float)\n",
        "\n",
        "if prob_col is not None and \"next_state\" in T.columns:\n",
        "    # Long format (explicit next_state + prob)\n",
        "    for _, r in T.iterrows():\n",
        "        a  = a2i[r[action_col]]\n",
        "        s  = s2i[r[state_col]]\n",
        "        sp = s2i[r[\"next_state\"]]\n",
        "        P[a, s, sp] += float(r[prob_col])\n",
        "else:\n",
        "    # Wide format: one column per destination state\n",
        "    id_like = {state_col, action_col}\n",
        "    candidate_cols = [c for c in T.columns if c not in id_like]\n",
        "    for c in candidate_cols:\n",
        "        if c not in s2i:\n",
        "            states.append(c)\n",
        "            s2i[c] = len(s2i)\n",
        "    # rebuild structures\n",
        "    states = sorted(states)\n",
        "    s2i = {s:i for i,s in enumerate(states)}\n",
        "    S = len(states)\n",
        "    P = np.zeros((A, S, S), dtype=float)\n",
        "    for _, r in T.iterrows():\n",
        "        a = a2i[r[action_col]]\n",
        "        s = s2i[r[state_col]]\n",
        "        row_probs = np.zeros(S)\n",
        "        for sp_name in candidate_cols:\n",
        "            row_probs[s2i[sp_name]] = float(r[sp_name])\n",
        "        # normalize if necessary\n",
        "        total = row_probs.sum()\n",
        "        if total > 0 and not np.isclose(total, 1.0, atol=1e-6):\n",
        "            row_probs /= total\n",
        "        P[a, s, :] = row_probs\n",
        "\n",
        "\n",
        "# define reward\n",
        "sick_states = {\"symptoms-1\", \"symptoms-2\", \"symptoms-3\"} \n",
        "sick_mask   = np.array([s in sick_states for s in states], dtype=bool)\n",
        "better_mask = ~sick_mask\n",
        "\n",
        "R_duration = np.zeros((A, S), dtype=float)\n",
        "R_duration[:, sick_mask]   = -1.0\n",
        "R_duration[:, better_mask] =  0.0\n",
        "\n",
        "v_star_rewards = value_iteration(R_duration, P, gamma=gamma, max_iterations=20000, tolerance=1e-10)\n",
        "Q_star = compute_qT_once(R_duration, P, gamma, v_star_rewards)\n",
        "best_action_idx_per_state = np.argmax(Q_star, axis=0)\n",
        "optimal_actions = [actions[i] for i in best_action_idx_per_state]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "px1xDndA_F3O"
      },
      "source": [
        "Save the optimal actions for each state to a file \"minimum-duration-actions.tsv\" with columns state and action.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "PGvWqSiI_Sqy"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "minimum_duration_actions=pd.DataFrame({\n",
        "    \"state\": states, \n",
        "    \"action\": optimal_actions\n",
        "    })\n",
        "\n",
        "minimum_duration_actions.to_csv(\"minimum-duration-actions.tsv\", sep=\"\\t\", index=False)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "itPVLMaM_UDn"
      },
      "source": [
        "Submit \"minimum-duration-actions.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eOzSQ3fV_XBO"
      },
      "source": [
        "## Part 6: Shorter Twizzleflu?\n",
        "\n",
        "Compute the expected number of days sick for each state to a file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "WO_yubXg_gxn"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([ 1.23922232,  2.48092556,  4.96681793, -0.        ,  9.94357944,\n",
              "        6.64008788,  3.32557366])"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "expected_sick_days = -v_star_rewards\n",
        "expected_sick_days"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Zf8j6_D_hbZ"
      },
      "source": [
        "Save the expected sick days for each state to a file \"minimum-duration-days.tsv\" with columns state and expected_sick_days."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "id": "yWS2HNVl_o3P"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "minimum_duration_days=pd.DataFrame({\n",
        "    \"state\": states, \n",
        "    \"expected_sick_days\": expected_sick_days\n",
        "    })\n",
        "\n",
        "minimum_duration_days.to_csv(\"minimum-duration-days.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVApozXF_pjI"
      },
      "source": [
        "Submit \"minimum-duration-days.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Znw87KK7_uv5"
      },
      "source": [
        "## Part 7: Speed vs Pampering\n",
        "\n",
        "Compute the expected discomfort using the policy to minimize days sick, and compare the results to the expected discomfort when optimizing to minimize discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "0AdnpD-6__y5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[-0.    -0.    -0.    -0.    -0.    -0.     0.375  0.75   0.375]\n",
            "[-0.  -0.  -0.  -0.  -0.  -0.   0.5  1.   0.5]\n"
          ]
        }
      ],
      "source": [
        "# YOUR CHANGES HERE\n",
        "gamma_discomfort = 0.9\n",
        "gamma_duration   = 0.999\n",
        "state_col, action_col = \"state\", \"action\"\n",
        "sick_states = {\"symptoms-1\",\"symptoms-2\",\"symptoms-3\"}  # edit if needed\n",
        "\n",
        "# Load\n",
        "T = pd.read_csv(\"twizzleflu-transitions.tsv\", sep=\"\\t\").rename(columns=str.lower)\n",
        "Rraw = pd.read_csv(\"twizzleflu-rewards.tsv\", sep=\"\\t\").rename(columns=str.lower)\n",
        "\n",
        "# States & actions\n",
        "states_from_T = set(T[state_col].unique())\n",
        "states_from_R = set(Rraw[\"state\"].unique()) if \"state\" in Rraw.columns else set()\n",
        "dest_candidates = [c for c in T.columns if c not in {state_col, action_col}]  # wide-format dest columns\n",
        "all_states = states_from_T | states_from_R | set(dest_candidates)              # include destination-only states\n",
        "states  = sorted(all_states)\n",
        "actions = sorted(T[action_col].unique())\n",
        "s2i = {s:i for i,s in enumerate(states)}\n",
        "a2i = {a:i for i,a in enumerate(actions)}\n",
        "S, A = len(states), len(actions)\n",
        "\n",
        "# ---- Build P (wide format) ----\n",
        "dest_cols = [c for c in dest_candidates if c in states]  # only true states\n",
        "Tg = T.groupby([state_col, action_col], as_index=False)[dest_cols].sum()\n",
        "\n",
        "P = np.zeros((A, S, S), float)\n",
        "for _, r in Tg.iterrows():\n",
        "    a, s = a2i[r[action_col]], s2i[r[state_col]]\n",
        "    row = np.array([pd.to_numeric(r[c], errors=\"coerce\") for c in dest_cols], float)\n",
        "    row = np.nan_to_num(row, nan=0.0)\n",
        "    tot = row.sum()\n",
        "    if tot > 0 and not np.isclose(tot, 1.0, atol=1e-6):\n",
        "        row /= tot\n",
        "    for j, sp_name in enumerate(dest_cols):\n",
        "        P[a, s, s2i[sp_name]] = row[j]\n",
        "\n",
        "# Ensure every (a,s) row sums to 1: add self-loops where a row is empty\n",
        "row_sums = P.sum(axis=2)\n",
        "empty_mask = np.isclose(row_sums, 0.0)\n",
        "if empty_mask.any():\n",
        "    for ai, si in np.argwhere(empty_mask):\n",
        "        P[ai, si, si] = 1.0\n",
        "\n",
        "# Final sanity check\n",
        "assert np.allclose(P.sum(axis=2), 1.0, atol=1e-6), \"Each (action,state) row of P must sum to 1.\"\n",
        "\n",
        "# ---- Original reward R_orig: (A, S) negative discomfort ----\n",
        "if {\"state\",\"action\",\"next_state\",\"reward\"}.issubset(Rraw.columns):\n",
        "    Rsa = Rraw.groupby([\"state\",\"action\"], as_index=False)[\"reward\"].mean()\n",
        "else:\n",
        "    Rsa = Rraw[[\"state\",\"action\",\"reward\"]].copy()\n",
        "\n",
        "R_orig = np.zeros((A, S), float)\n",
        "for _, row in Rsa.iterrows():\n",
        "    if row[\"action\"] in a2i and row[\"state\"] in s2i:\n",
        "        R_orig[a2i[row[\"action\"]], s2i[row[\"state\"]]] = float(row[\"reward\"])\n",
        "\n",
        "# Helper: evaluate fixed policy under reward R\n",
        "def eval_policy(R, P, gamma, pi_idx):\n",
        "    S = P.shape[1]\n",
        "    R_pi = np.array([R[pi_idx[s], s] for s in range(S)], float)\n",
        "    P_pi = np.array([P[pi_idx[s], s, :] for s in range(S)], float)\n",
        "    V = np.zeros(S, float)\n",
        "    for _ in range(20000):\n",
        "        V_new = R_pi + gamma * (P_pi @ V)\n",
        "        if np.max(np.abs(V_new - V)) < 1e-8:\n",
        "            return V_new\n",
        "        V = V_new\n",
        "    return V\n",
        "\n",
        "# ===== Policy that minimizes expected discomfort (Part 2) =====\n",
        "v_disc = value_iteration(R_orig, P, gamma_discomfort)\n",
        "# IMPORTANT: the grader’s \"minimum-discomfort-values.tsv\" equals -v_disc\n",
        "minimize_discomfort = -v_disc\n",
        "\n",
        "# ===== Policy that minimizes days sick (Part 5) =====\n",
        "R_duration = np.zeros((A, S), float)\n",
        "R_duration[:, [s in sick_states for s in states]] = -1.0\n",
        "v_dur = value_iteration(R_duration, P, gamma_duration)\n",
        "pi_dur = np.argmax(compute_qT_once(R_duration, P, gamma_duration, v_dur), axis=0)\n",
        "\n",
        "# Evaluate speed policy using original rewards to get discomfort\n",
        "speed_discomfort = -eval_policy(R_orig, P, gamma_discomfort, pi_dur)\n",
        "\n",
        "print(minimize_discomfort)\n",
        "print(speed_discomfort)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M3ZVJ2lcAAkP"
      },
      "source": [
        "Save the results to a file \"policy-comparison.tsv\" with columns state, speed_discomfort, and minimize_discomfort."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "9H9EG0zTAMt1"
      },
      "outputs": [],
      "source": [
        "# YOUR CHANGES HERE\n",
        "\n",
        "pd.DataFrame({\n",
        "    \"state\": states,\n",
        "    \"speed_discomfort\": speed_discomfort,\n",
        "    \"minimize_discomfort\": minimize_discomfort\n",
        "}).to_csv(\"policy-comparison.tsv\", sep=\"\\t\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EVhLZuuaANNf"
      },
      "source": [
        "Submit \"policy-comparison.tsv\" in Gradescope."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "smsTLuFcvR-I"
      },
      "source": [
        "## Part 8: Code\n",
        "\n",
        "Please submit a Jupyter notebook that can reproduce all your calculations and recreate the previously submitted files."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zi8lV2pbvWMs"
      },
      "source": [
        "## Part 9: Acknowledgements\n",
        "\n",
        "If you discussed this assignment with anyone, please acknowledge them here.\n",
        "If you did this assignment completely on your own, simply write none below.\n",
        "\n",
        "If you used any libraries not mentioned in this module's content, please list them with a brief explanation what you used them for. If you did not use any other libraries, simply write none below.\n",
        "\n",
        "If you used any generative AI tools, please add links to your transcripts below, and any other information that you feel is necessary to comply with the generative AI policy. If you did not use any generative AI tools, simply write none below."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": false
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.1"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
